{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52da5e4e-97ee-454c-864f-abd1286e3f24",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "---\n",
    "\n",
    "This model ignores the review text and carries out predictions only based on the marginal probabilities of labels.\n",
    "\n",
    "Always predicting the most common wine variety is the best we can do if we completely ignore the text in the `description`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaccc51-df26-4cc6-a948-a664594207ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f4b6ce-78d8-4aa9-b90b-fce19080800c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def warn(string):\n",
    "    display(Markdown('<span style=\"color:red\">'+string+'</span>'))\n",
    "def info(string):\n",
    "    display(Markdown('<span style=\"color:blue\">'+string+'</span>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2337f9cd-fd87-4538-b71b-c6aa53ab773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.cores\", \"16\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .appName(\"jojoSparkSession\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.default.parallelism\", \"16\") \\\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5fc136-a068-4d5f-bf67-4b7fa20b5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|index|         description|           variety|\n",
      "+-----+--------------------+------------------+\n",
      "|    0|aromas include tr...|       white blend|\n",
      "|    1|this is ripe and ...|    portuguese red|\n",
      "|    2|tart and snappy, ...|        pinot gris|\n",
      "|    3|pineapple rind, l...|          riesling|\n",
      "|    4|much like the reg...|        pinot noir|\n",
      "|    7|this dry and rest...|    gewürztraminer|\n",
      "|    8|savory dried thym...|    gewürztraminer|\n",
      "|    9|this has great de...|        pinot gris|\n",
      "|   10|soft, supple plum...|cabernet sauvignon|\n",
      "|   11|this is a dry win...|    gewürztraminer|\n",
      "|   12|slightly reduced,...|cabernet sauvignon|\n",
      "|   14|building on 150 y...|        chardonnay|\n",
      "|   15|zesty orange peel...|          riesling|\n",
      "|   16|baked plum, molas...|            malbec|\n",
      "|   17|raw black-cherry ...|            malbec|\n",
      "|   18|desiccated blackb...| tempranillo blend|\n",
      "|   19|red fruit aromas ...|          meritage|\n",
      "|   20|ripe aromas of da...|         red blend|\n",
      "|   21|a sleek mix of ta...|        pinot noir|\n",
      "|   22|delicate aromas r...|       white blend|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "reviews_sdf = spark.read.parquet('data/reviews_cleaned')\n",
    "reviews_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80066656-b1e5-4af6-8118-3802cb2341a3",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "Since we are not using any features for this, there is really noting to do here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d26e20-9e30-4d78-963c-121db5025e96",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We want to predict the label based on the frequency of labels in our training dataset.\n",
    "\n",
    "So the steps we need to do:\n",
    "\n",
    "1. Convert the wine variety column to a categorical label.\n",
    "2. Identify the most common label and use it as prediction.\n",
    "\n",
    "_See the [README.md](./README.md#Model-Definition) for further details._\n",
    "\n",
    "We will implement both steps using sprak Transformers and Estimators.\n",
    "\n",
    "For **step 1** we can readily use pyspark's [StringIndexer](https://spark.apache.org/docs/3.1.1/ml-features.html#stringindexer).\n",
    "\n",
    "**Step 2** needs a some more doing from our side.\n",
    "We implement a custom estimator `MarginalMaximizer` by sub-classing pyspark's [Estimator](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.Estimator.html)\n",
    "The implementation is found in [utils/Estimators.py](./utils/Estimators.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc596ac0-788f-4729-9afc-5b5f0926e25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initiate our string indexer\n",
    "si = StringIndexer(inputCol='variety', outputCol='label', handleInvalid='keep')\n",
    "\n",
    "# import our custom estimator\n",
    "from utils.Estimators import MarginalMaximizer\n",
    "# and initiate it\n",
    "mm = MarginalMaximizer(inputCol='label', outputCol='prediction')\n",
    "\n",
    "# all we need to do is setting up a pipeline:\n",
    "marginal_ppl = Pipeline(stages=[si, mm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f0998-d495-43e3-a1af-87605a0ae5a0",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we can start 'training' our baseline model.\n",
    "\n",
    "But first, split our data into a training and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d39955-68bd-4edd-99a4-233b795025d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = reviews_sdf.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7abd51e-f1e4-40a6-855e-f8199a250186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 ms, sys: 12.5 ms, total: 28.7 ms\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ###\n",
    "# This is simply to get an idea of how long it takes to run\n",
    "# Since pipelines are lazy we add the df.show() at the end\n",
    "# to force execution (otherwise timing would not make much sense)\n",
    "# ###\n",
    "marginal_model = marginal_ppl.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c21edf-9bd0-4182-835c-bf88155292fe",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "With `marginal_model` being fitted to the train data we can now check its performance, both on the training and on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf5a2f3-91d7-4e06-a5cb-4e2b2a8f13b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+-----+----------+\n",
      "|index|         description|           variety|label|prediction|\n",
      "+-----+--------------------+------------------+-----+----------+\n",
      "|    0|aromas include tr...|       white blend| 15.0|       0.0|\n",
      "|    1|this is ripe and ...|    portuguese red| 14.0|       0.0|\n",
      "|    2|tart and snappy, ...|        pinot gris| 19.0|       0.0|\n",
      "|    3|pineapple rind, l...|          riesling|  5.0|       0.0|\n",
      "|    4|much like the reg...|        pinot noir|  0.0|       0.0|\n",
      "|    7|this dry and rest...|    gewürztraminer| 27.0|       0.0|\n",
      "|    9|this has great de...|        pinot gris| 19.0|       0.0|\n",
      "|   10|soft, supple plum...|cabernet sauvignon|  2.0|       0.0|\n",
      "|   11|this is a dry win...|    gewürztraminer| 27.0|       0.0|\n",
      "|   12|slightly reduced,...|cabernet sauvignon|  2.0|       0.0|\n",
      "|   14|building on 150 y...|        chardonnay|  1.0|       0.0|\n",
      "|   15|zesty orange peel...|          riesling|  5.0|       0.0|\n",
      "|   16|baked plum, molas...|            malbec| 13.0|       0.0|\n",
      "|   17|raw black-cherry ...|            malbec| 13.0|       0.0|\n",
      "|   18|desiccated blackb...| tempranillo blend| 37.0|       0.0|\n",
      "|   19|red fruit aromas ...|          meritage| 52.0|       0.0|\n",
      "|   20|ripe aromas of da...|         red blend|  3.0|       0.0|\n",
      "|   21|a sleek mix of ta...|        pinot noir|  0.0|       0.0|\n",
      "|   22|delicate aromas r...|       white blend| 15.0|       0.0|\n",
      "|   23|this wine from th...|            merlot|  9.0|       0.0|\n",
      "+-----+--------------------+------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = marginal_model.transform(df_train)\n",
    "pred_train.show()\n",
    "# nbr_labels = int(pred_train.agg({'label': 'max'}).collect()[0]['max(label)'])\n",
    "# info(f'We are dealing with {nbr_labels} different labels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df984c36-2f9b-4b2f-8ea6-6ac99c1201cd",
   "metadata": {},
   "source": [
    "We use the **accuracy** as evaluation metric as it indicates the fraction of labels that were correctly guessed.\n",
    "In this baseline model the predicted labels are for all samples the same, i.e. the most common label in the training data.\n",
    "It follows that the accuracy will indicate the marginal probability of this label.\n",
    "\n",
    "Now we define our evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fc5ed7-d31b-4876-945c-aa3ef2f91eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\n",
    "                                             metricName='accuracy')\n",
    "f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction',\n",
    "                                       metricName='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686cb27e-18d6-45d2-9555-bf85dc4eb998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">How well is the marginal prediction doing?:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">Accuracy: **11.35%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">F1 score: **0.0231**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info('How well is the marginal prediction doing?:')\n",
    "info(f'Accuracy: **{round(accuracy.evaluate(pred_train), 4)*100}%**')\n",
    "info(f'F1 score: **{round(f1.evaluate(pred_train), 4)}**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c42da48-2d47-4104-bacb-e2e64e545cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">How well are we dong on the test dataset?:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">Accuracy: **11.3%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">F1 score: **0.023**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info('How well are we dong on the test dataset?:')\n",
    "pred_test = marginal_model.transform(df_test)\n",
    "info(f'Accuracy: **{round(accuracy.evaluate(pred_test), 4)*100}%**')\n",
    "info(f'F1 score: **{round(f1.evaluate(pred_test), 4)}**')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb0edd-ff0e-4744-9e75-1d21040b5b97",
   "metadata": {},
   "source": [
    "Now we have a baseline:\n",
    "\n",
    "**An accuracy of around 11% can be obtained without considering the text in the `description` column.**\n",
    "\n",
    "Any model that performs better than that is capable to retrieve some information from the review text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd4ef0-e4b6-4f6e-8f6f-fae601deeb5a",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Since this is a rather simple model we will save it here directly in a deployable form.\n",
    "\n",
    "Have a look at [reviewed_grapes.model_deployment.pyspark.v1.ipynb](reviewed_grapes.model_deployment.pyspark.v1.ipynb) for details on the deployment process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec3403f-f647-4a92-af09-9ef43df0318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the StringIndexer\n",
    "s_first = marginal_model.stages.pop(0)\n",
    "# get the labels from the string indexer\n",
    "labels = s_first.labels\n",
    "# now we construct a index->string\n",
    "s_last = marginal_model.stages[-1]\n",
    "its = IndexToString(inputCol=s_last.getOutputCol(),\n",
    "                    outputCol='_prediction',\n",
    "                    labels=labels)\n",
    "marginal_model.stages.append(its)\n",
    "# the model is ready for deployment\n",
    "name = 'reviewed_grapes/fitted_models/MarginalModel'\n",
    "marginal_model.write().overwrite().save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8286e-b4f2-4766-a275-1478edd9337f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
