{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef359050-0948-401b-833a-ee6999a59f66",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "---\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "We want to use the text in the `description` column to predict the wine variety.\n",
    "\n",
    "We identified the general steps to do this (_see the [README.md](./README.md#Model-Definition) for further details_):\n",
    "\n",
    "1. _step1_: Convert the wine variety column to a categorical label.\n",
    "2. _step2_: Render each word in the review text to a canonical form.\n",
    "3. _step3_: Use a predefined set of words, a _target word set_, to create a feature vector of binary features, each indicating the presence or absence of one of the words in the target word set.\n",
    "4. Define a ML model that can predict the label based on these features.\n",
    "5. Train the model on a training set and assess its performance.\n",
    "6. Deploy the model.\n",
    "\n",
    "\n",
    "Loosely put we want to define our model in the form of a pipeline:\n",
    "\n",
    "    pipeline = Pipeline(stages=[step1, step2, step3, step4])\n",
    "    \n",
    "To do this we need to define _step1_, _step2_, _step3_ and decide which method to use for _step4_.\n",
    "\n",
    "Luckily for _step1_ this is straight forward:\n",
    "\n",
    "**Step 1** is identical to our [baseline model](./reviwed_grapes.baseline.method_def.pyspark.v1.ipynb) and done with pyspark's [StringIndexer](https://spark.apache.org/docs/3.1.1/ml-features.html#stringindexer).\n",
    "\n",
    "For **step 2** and **step 3** it is unfortunately not that simple.\n",
    "Here we need to do some feature engineering and define custom Transformers and Models that we then can include into our pipeline.\n",
    "These steps are carried out and illustrated in the [feature engineering notebook](reviewed_grapes.feature_eng.pyspark.v1.ipynb).\n",
    "\n",
    "According to the approach we decided to take, the method used in **step 4** should be a traditional ML method.\n",
    "We have several options here and which one we pick should depend on how well they perform.\n",
    "In the [model evaluation notebook](reviewed_grapes.model_evaluation.pyspark.v1.ipynb) we have tested several models.\n",
    "It turns out that [Multinomial Logistic Regression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html) performs best in our case, so we will use this method for step 4.\n",
    "\n",
    "Here below we will proceed with the model definition and use our finding for steps 2, 3 and 4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda18ed4-98bc-41b2-ac7d-83028ba15a98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install nltk\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d069d53-6bca-4151-8e47-b618426c2193",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d6d7f6-3d1d-4f7e-bf88-3039db774351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jonas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1451965-1773-42f1-8822-977c691d2fd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def warn(string):\n",
    "    display(Markdown('<span style=\"color:red\">'+string+'</span>'))\n",
    "def info(string):\n",
    "    display(Markdown('<span style=\"color:blue\">'+string+'</span>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5212fc9-1df3-43b8-85ed-511fae859e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    .appName(\"jojoSparkSession\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"20g\") \\\n",
    "    # .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    # .config(\"spark.executor.cores\", \"16\") \\\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e843828-1be2-4bc6-8bd9-b19756bc6e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|index|         description|           variety|\n",
      "+-----+--------------------+------------------+\n",
      "|    0|aromas include tr...|       white blend|\n",
      "|    1|this is ripe and ...|    portuguese red|\n",
      "|    2|tart and snappy, ...|        pinot gris|\n",
      "|    3|pineapple rind, l...|          riesling|\n",
      "|    4|much like the reg...|        pinot noir|\n",
      "|    7|this dry and rest...|    gewürztraminer|\n",
      "|    8|savory dried thym...|    gewürztraminer|\n",
      "|    9|this has great de...|        pinot gris|\n",
      "|   10|soft, supple plum...|cabernet sauvignon|\n",
      "|   11|this is a dry win...|    gewürztraminer|\n",
      "|   12|slightly reduced,...|cabernet sauvignon|\n",
      "|   14|building on 150 y...|        chardonnay|\n",
      "|   15|zesty orange peel...|          riesling|\n",
      "|   16|baked plum, molas...|            malbec|\n",
      "|   17|raw black-cherry ...|            malbec|\n",
      "|   18|desiccated blackb...| tempranillo blend|\n",
      "|   19|red fruit aromas ...|          meritage|\n",
      "|   20|ripe aromas of da...|         red blend|\n",
      "|   21|a sleek mix of ta...|        pinot noir|\n",
      "|   22|delicate aromas r...|       white blend|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "reviews_sdf = spark.read.parquet('data/reviews_cleaned')\n",
    "reviews_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fecd2c-8ee1-49fa-9ff5-03db86baea91",
   "metadata": {},
   "source": [
    "**Now we can start defining our models**\n",
    "\n",
    "**Step 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a76424b-1033-467d-a3e5-95ed11dafc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol='variety', outputCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db51bc9-961d-4db9-b32d-d872d63d447f",
   "metadata": {},
   "source": [
    "**Step 2:**\n",
    "\n",
    "Let's import our custom transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d81e69-1563-46c8-94f3-050558eccf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jonas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jonas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from reviewed_grapes.transformers import NLTKLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a3846d-3707-4bb7-aa6a-9447e8ce0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common english stopwords:\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "# we also define a custom blacklist:\n",
    "blacklist = set(['wine', 'drink', 'variety', 'show', 'offer', 'make', 'give', 'well',\n",
    "                 'open', 'come', 'years', 'bottle', 'mouth', 'like', 'also', 'along',\n",
    "                 'alongside', 'vineyard', 'ready', 'great', 'one', 'slightly', 'deliver',\n",
    "                 'yet', 'add', 'need', 'big', 'bring', 'easy', 'oral', 'best', 'end', 'alcohol'])\n",
    "# complete the list with our custom blacklist:\n",
    "stop_words = list(en_stopwords.union(blacklist))\n",
    "# instatiate our transformer and the estimator:\n",
    "nltkl = NLTKLemmatizer(inputCol='description', outputCol='words', stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11db49-0fb1-426e-8273-539c6cf58093",
   "metadata": {},
   "source": [
    "**Step 3:**\n",
    "\n",
    "Import our custom estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd96490-a978-4f58-9199-7778df98427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Estimators import WordSetTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59b322-0d34-45c0-a9c5-f862bdcd3eb6",
   "metadata": {},
   "source": [
    "Finally, lets load the different _target word sets_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee40a53a-c43f-43bc-a476-f4843aa02bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/targetWordSets/common_words.json', 'r') as fo:\n",
    "    common_words = json.load(fo)\n",
    "with open('data/targetWordSets/similar_words.json', 'r') as fo:\n",
    "    similar_words = json.load(fo)\n",
    "with open('data/targetWordSets/dissimilar_words.json', 'r') as fo:\n",
    "    dissimilar_words = json.load(fo)\n",
    "with open('data/targetWordSets/extreme_words.json', 'r') as fo:\n",
    "    extremes_words = json.load(fo)\n",
    "with open('data/targetWordSets/lowentropy_words.json', 'r') as fo:\n",
    "    lowentropy_words = json.load(fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74229aa-a69b-4d6f-8714-320f2dc2eb5d",
   "metadata": {},
   "source": [
    "Here we use the different feature sets (target word sets) with the best performing length (14\\* 57 _refer to the [crossvalidation part of the model evaluation](reviewed_grapes.model_evaluation.pyspark.v1.ipynb#Target-Word-Set-Evaluation) for details_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b17beaa-d816-4b28-97c1-f138a31646ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limit_to = 14 * 57\n",
    "# initiate the estimators for our 5 different models:\n",
    "wst_common = WordSetTracker(inputCol='words', outputCol='features', wordSet=common_words, limitTo=limit_to)\n",
    "wst_similar = WordSetTracker(inputCol='words', outputCol='features', wordSet=similar_words, limitTo=limit_to)\n",
    "wst_dissimilar = WordSetTracker(inputCol='words', outputCol='features', wordSet=dissimilar_words, limitTo=limit_to)\n",
    "wst_extremes = WordSetTracker(inputCol='words', outputCol='features', wordSet=extremes_words, limitTo=limit_to)\n",
    "wst_lowentropy = WordSetTracker(inputCol='words', outputCol='features', wordSet=lowentropy_words, limitTo=limit_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfb81a-b105-4dbf-993f-27a264140fed",
   "metadata": {},
   "source": [
    "**Step 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699cc114-11e8-4e19-824d-4343c9f9fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4ec79-6535-4142-97f6-b7c41387b33b",
   "metadata": {},
   "source": [
    "And finally we initiate the logistic regression.\n",
    "Note that the values for `regParam` and `elasticNetParam` come from the cross validation carried out in [model evaluation notebook](reviewed_grapes.model_evaluation.pyspark.v1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ff0c545-4e00-465e-a410-7f269c081c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlrc = LogisticRegression(regParam=0.01, elasticNetParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8d83e-0079-49ee-bbed-866ad61e2e95",
   "metadata": {},
   "source": [
    "Now here are the pipelines for our models.\n",
    "\n",
    "We directly put them into a `dict` such that we can loop over for training.\n",
    "\n",
    "_Note: The keys will also be the final names of the models when saved._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85557a5c-eb0f-42b2-a6ec-b6d3412310fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipelines = dict(\n",
    "    CommonWordsModel = Pipeline(stages=[si, nltkl, wst_common, mlrc]),\n",
    "    SimilarWordsModel = Pipeline(stages=[si, nltkl, wst_similar, mlrc]),\n",
    "    DissimilarWordsModel = Pipeline(stages=[si, nltkl, wst_dissimilar, mlrc]),\n",
    "    ExtremesWordsModel = Pipeline(stages=[si, nltkl, wst_extremes, mlrc]),\n",
    "    LowentropyWordsModel = Pipeline(stages=[si, nltkl, wst_lowentropy, mlrc])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f39b9-2dda-46e5-a865-547c2021c8b6",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Training\n",
    "\n",
    "With our models defined we can train them.\n",
    "\n",
    "Recall that we carry out model evaluation in the [model evaluation notebook](reviewed_grapes.model_evaluation.pyspark.v1.ipynb) and thus the model training we perform here happens with already tested and optimized hyperparameters.\n",
    "Nevertheless we perform a train/test split of our data, just to be sure that the training really works as expected.\n",
    "\n",
    "As a first step we define a training and a test set to be able to check for overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88d39955-68bd-4edd-99a4-233b795025d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = reviews_sdf.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad30389-4402-4649-811f-29b725c7679a",
   "metadata": {},
   "source": [
    "Also, we need to define some evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55cf9c19-a27e-44b6-9beb-50cb59d4def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "f1 = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6d209-30d5-495a-8622-514576287d44",
   "metadata": {},
   "source": [
    "Now we're ready to train our models.\n",
    "\n",
    "For each model the steps are as follows:\n",
    "\n",
    "  1. Fitting the pipeline to get a model\n",
    "  2. Evaluate the fitted model with the test ds.\n",
    "  3. Export the model for further processing.\n",
    "\n",
    "Note: After step 3 our models still need to be made ready for deployment.\n",
    "This final step is explained and carried out in the [model deployment notebook](reviewed_grapes.model_deployment.pyspark.v1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "850edc07-2247-41d5-bc17-b041445f6011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Processing model \"CommonWordsModel\"**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Training data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **55.8%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.5248**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Test data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **52.0%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4836**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Processing model \"SimilarWordsModel\"**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Training data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **52.1%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4854**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Test data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **48.9%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.452**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Processing model \"DissimilarWordsModel\"**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Training data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **49.7%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4593**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Test data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **46.2%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4225**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Processing model \"ExtremesWordsModel\"**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Training data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **52.4%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4869**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Test data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **48.8%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4494**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Processing model \"LowentropyWordsModel\"**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Training data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **53.3%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4972**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">  Test data:</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - Accuracy: **50.0%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">    - F1 score: **0.4623**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 926 ms, sys: 249 ms, total: 1.18 s\n",
      "Wall time: 43min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for model_name, ppl in model_pipelines.items():\n",
    "    info(f'**Processing model \"{model_name}\"**')\n",
    "    # fit the pipeline\n",
    "    fitted_model = ppl.fit(df_train)\n",
    "    # transform the training data\n",
    "    pred_train = fitted_model.transform(df_train)\n",
    "    # transform the test data\n",
    "    pred_test = fitted_model.transform(df_test)\n",
    "    # now check for overfitting \n",
    "    info('  Training data:')\n",
    "    info(f'    - Accuracy: **{round(accuracy.evaluate(pred_train)*100, 1)}%**')\n",
    "    info(f'    - F1 score: **{round(f1.evaluate(pred_train), 4)}**')\n",
    "    info('  Test data:')\n",
    "    info(f'    - Accuracy: **{round(accuracy.evaluate(pred_test)*100, 1)}%**')\n",
    "    info(f'    - F1 score: **{round(f1.evaluate(pred_test), 4)}**')\n",
    "    # and finally save it \n",
    "    path = 'data/interim/' + model_name\n",
    "    fitted_model.write().overwrite().save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f79377-434f-4d1c-8995-4b69ef84393a",
   "metadata": {},
   "source": [
    "We are done with training our models with optimized hyperparameters.\n",
    "All the remains to do now is getting them ready for deployment.\n",
    "\n",
    "This final step is explained and carried out in the [model deployment notebook](reviewed_grapes.model_deployment.pyspark.v1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1eceb-b49e-4e7a-9641-bce8ce4b1271",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
